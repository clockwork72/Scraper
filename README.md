# Privacy Research Dataset + Dashboard

This repository builds a **Step‑1 privacy research dataset** and ships an **Electron dashboard** to run and inspect crawls.

## What this project does

**Scraper (Python)**
- Website → first‑party privacy policy URL + extracted text
- Website → observed third‑party domains (from network requests)
- Third‑party domain → entity/category/policy URL (DuckDuckGo Tracker Radar or Ghostery TrackerDB)
- Third‑party policy URL → extracted policy text (best‑effort)

**Dashboard (Electron + Vite)**
- Launch the scraper with live progress + logs
- Inspect results, entities, categories, and prevalence
- Browse sites + policies via the Explorer
- Run history and per‑run output folders
- Mapping mode selection (Tracker Radar / TrackerDB / Mixed)
- Clear results/artifacts from the database view

**No LLMs / DeepSeek.** The pipeline is deterministic + heuristic filtering.

---

## Repository layout

- `privacy_research_dataset/` — core scraper package
- `scripts/` — helper scripts (Tracker Radar/TrackerDB index, Tranco fetch)
- `tracker-radar/` — DuckDuckGo Tracker Radar repo (clone here)
- `trackerdb/` — Ghostery TrackerDB repo (clone here, optional)
- `dashboard/` — Electron + Vite UI
- `outputs/` — per‑run outputs (`outputs/output_<runid>/`)

---

## Quick start (scraper only)

### 1) Python setup

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Crawl4AI uses Playwright
python -m playwright install chromium
```

### 2) Tracker Radar index

```bash
git clone https://github.com/duckduckgo/tracker-radar.git tracker-radar
python scripts/build_tracker_radar_index.py --tracker-radar-dir tracker-radar --out tracker_radar_index.json
```

### 3) Ghostery TrackerDB index (optional)

```bash
git clone https://github.com/ghostery/trackerdb trackerdb
python scripts/build_trackerdb_index.py --trackerdb-dir trackerdb --out trackerdb_index.json
```

### 4) Run a crawl

```bash
privacy-dataset --tranco-top 100 --tranco-date 2026-01-01 \
  --tracker-radar-index tracker_radar_index.json \
  --trackerdb-index trackerdb_index.json \
  --out outputs/results.jsonl \
  --artifacts-dir outputs/artifacts
```

---

## Dashboard setup (Electron + Vite)

### Requirements
- Node.js 18+ (recommended)
- Python + scraper dependencies installed

### Install & run

```bash
cd dashboard
npm install
npm run dev
```

The dashboard can **start the scraper directly** via IPC. If your Python is not `python` on PATH, set:

```bash
export PRIVACY_DATASET_PYTHON=/path/to/python
```

---

## Dashboard → Scraper integration

When launched from the dashboard, each run is stored under:

```
outputs/output_<runid>/
```

Each run folder contains:

- `results.jsonl` — raw results
- `results.summary.json` — aggregated summary (includes mapping mode + counts)
- `run_state.json` — live run counters (includes mapping counters)
- `explorer.jsonl` — explorer data

These are produced when you run with:

```bash
privacy-dataset \
  --emit-events \
  --state-file outputs/output_<runid>/run_state.json \
  --summary-out outputs/output_<runid>/results.summary.json \
  --explorer-out outputs/output_<runid>/explorer.jsonl \
  --out outputs/output_<runid>/results.jsonl \
  --artifacts-dir outputs/output_<runid>/artifacts
```

The Electron app uses these files to power:
- **Results tab** (summary + categories + entities)
- **Explorer tab** (sites + policy links)
- **Analytics tab** (run state)
- **Database tab** (run history + load/delete runs)

---

## Scraper CLI options (important)

- `--tranco-top N` / `--tranco-date YYYY-MM-DD` — reproducible Tranco list
- `--tracker-radar-index` — enables entity/category mapping via Tracker Radar
- `--trackerdb-index` — enables entity/category mapping via Ghostery TrackerDB (used as fallback if Tracker Radar misses)
- `--third-party-engine crawl4ai|openwpm` — network collection
- `--no-third-party-policy-fetch` — disable third‑party policy fetch

**Integration / telemetry**
- `--emit-events` — JSON events to stdout
- `--state-file` — run state JSON
- `--summary-out` — aggregated summary JSON
- `--explorer-out` — explorer JSON/JSONL
- `--run-id` — set a fixed run id

**CrUX filter (browsable origins)**
- `--crux-filter` — keep only sites present in Chrome UX Report
- `--crux-api-key` or `CRUX_API_KEY` env var
- `--crux-concurrency`, `--crux-timeout-ms`

**Entity filtering**
- `--exclude-same-entity` — exclude third‑party domains owned by same entity as first‑party (requires a mapping index)

**Browsable-only (optional)**
- `--prefilter-websites` — lightweight HTML check before crawl
- `--skip-home-fetch-failed` — do not write results when home fetch fails

---

## Output schema (high‑level)

Each line in `results.jsonl` contains:
- `status`: `ok`, `policy_not_found`, `non_browsable`, `home_fetch_failed`, `exception`
- `first_party_policy`: URL + score + length
- `third_parties`: eTLD+1 + entity + categories + prevalence + policy_url
- `third_parties`: may include `tracker_radar_source_domain_file` and `trackerdb_source_*` fields
- timing fields: `home_fetch_ms`, `policy_fetch_ms`, `third_party_extract_ms`, `third_party_policy_fetch_ms`, `total_ms`
- `run_id`, `started_at`, `ended_at`

Artifacts live under `outputs/output_<runid>/artifacts/<site>/`.

---

## Dashboard features (summary)

- **Launcher**: Tranco Top‑N, CrUX filter, mapping mode (Radar / TrackerDB / Mixed), exclude same‑entity, start/stop run.
- **Live progress**: step indicator, ETA, streaming logs, and “Open full log” in its own window.
- **Results**: split site vs third‑party stats, tooltips, status bar legend, mapping mode + mixed counts.
- **Explorer**: gallery view, search + status + minimum 3P filter, embedded policy viewer with “Open in window” fallback.
- **Database**: run history table, load any run, show folder size, clear/delete outputs.
- **Settings**: themes (dark, VS Code red, academia).

---

## Troubleshooting

**CrUX returns 403/401**
- Ensure Chrome UX Report API is enabled for your key
- Check referrer / IP restrictions in Google Cloud

**Dashboard cannot start scraper**
- Set `PRIVACY_DATASET_PYTHON` to correct Python
- Ensure `privacy_research_dataset` is importable in that environment

**No results in Explorer**
- Ensure `--explorer-out outputs/explorer.jsonl` is used
- The dashboard expects JSONL records with `site`, `rank`, `policyUrl`, and `thirdParties`

---

## Optional: OpenWPM

```bash
privacy-dataset --tranco-top 1000 --tranco-date 2026-01-01 \
  --tracker-radar-index tracker_radar_index.json \
  --third-party-engine openwpm --concurrency 1 \
  --out outputs/results.jsonl --artifacts-dir outputs/artifacts
```

OpenWPM is heavier and typically installed via its Docker/conda instructions.
